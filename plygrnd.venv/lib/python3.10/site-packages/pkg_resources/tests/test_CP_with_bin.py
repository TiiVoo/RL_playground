import unittest

import numpy as np

from CP_with_bin import CartpoleBin


class TestCartpoleBin(unittest.TestCase):
    def setUp(self):
        n_intp = 10
        self.interpolation_table_y = [np.linspace(0, 9, n_intp),
                                      np.linspace(0, 9, n_intp),
                                      np.linspace(0, 9, n_intp),
                                      np.linspace(0, 9, n_intp)]
        # position, speed, angle, angular speed
        self.interpolation_table_x = [np.linspace(-2.4, 2.4, n_intp),
                                      np.linspace(-10, 10, n_intp),
                                      np.linspace(-.2095, .2095, n_intp),
                                      np.linspace(-2.095, 2.095, n_intp)]
        epsilon = 0.1
        alpha = 0.1
        self.cp = CartpoleBin('rendermode0',
                              self.interpolation_table_y,
                              self.interpolation_table_x,
                              epsilon, alpha)


class TestInit(TestCartpoleBin):
    def test_init(self):
        action = 1
        self.cp._single_step(action)
        self.assertEqual(len(self.cp.interp_fct), len(self.cp.observation))

    def test_single_step(self):
        action = 1
        self.cp._single_step(action)
        self.assertEqual(len(self.cp.interp_fct), len(self.cp.observation))

    def test_box_output_int(self):

        n_support_points = len(self.interpolation_table_y[0])

        for count in range(n_support_points):
            observation = []
            result = []
            for x in self.interpolation_table_x:
                observation.append(x[count])
            for y in self.interpolation_table_y:
                result.append(y[count])
            self.cp._box_observation(observation)
            self.assertEqual(self.cp.obs_boxed, result)

    def test_box_output_out_of_bounds(self):
        observation = [1000, 1000, 1000, 1000]
        self.cp._box_observation(observation)
        self.assertEqual(self.cp.obs_boxed, [9, 9, 9, 9])
        observation = [-1000, -1000, -1000, -1000]
        self.cp._box_observation(observation)
        self.assertEqual(self.cp.obs_boxed, [0, 0, 0, 0])

    def test_find_state(self):
        observation = [1, 2, 3, 4]
        self.cp._find_new_state(observation)
        self.assertEqual(self.cp.state_new, 1234)

    def test_choose_action_epsilon_greedy(self):
        self.cp.epsilon = 0

        statelist = [1, 2000, 3001]
        for state in statelist:
            self.cp.epsilon = 0
            self.cp._choose_action_epsilon_greedy(state)

            if self.cp.Q[state][0] > self.cp.Q[state][1]:
                self.assertEqual(self.cp.action, 0)
            else:
                self.assertEqual(self.cp.action, 1)

    def test_choose_action_epsilon_greedy_epsilon(self):
        state = 1
        self.cp.epsilon = 1
        sum = 0
        iter = 1000
        for i in range(iter):
            self.cp._choose_action_epsilon_greedy(state)
            sum += self.cp.action

        message = "epsilon step statistically not clean"
        self.assertAlmostEqual(sum / iter, 0.5, None, message, 0.1)

    def test_q_learning(self):
        self.cp.action = 1
        self.cp._single_step( self.cp.action)
        self.cp.action = 0
        self.cp._single_step( self.cp.action)


        self.cp._update_Q()
        self.assertIsNotNone(self.cp.Q[self.cp.state][self.cp.action])
        self.assertTrue(type(self.cp.Q[self.cp.state][self.cp.action]) is np.float64)

        oldQ = self.cp.Q[self.cp.state][self.cp.action]
        self.cp.alpha = 0
        self.cp._update_Q()
        self.assertEqual(self.cp.Q[self.cp.state][self.cp.action],
                         oldQ)

        oldQ = self.cp.Q[self.cp.state][self.cp.action]
        self.cp.alpha = 1
        self.cp.epsilon = 0
        self.cp.reward = 0.5
        self.cp._update_Q()
        self.assertEqual(self.cp.Q[self.cp.state][self.cp.action],
                          self.cp.reward )

        oldQ = self.cp.Q[self.cp.state][self.cp.action]
        self.cp.alpha = 0.1
        self.cp.reward = 1
        self.cp._update_Q()
        self.assertEqual(self.cp.Q[self.cp.state][self.cp.action],
                         oldQ + self.cp.alpha * (self.cp.reward-oldQ)  )

        oldQ = self.cp.Q[self.cp.state][self.cp.action]
        self.cp.alpha = 1
        self.cp.epsilon = 1
        self.cp._update_Q()
        self.assertAlmostEqual(self.cp.Q[self.cp.state][self.cp.action],
                          oldQ+self.cp.reward+np.max(self.cp.Q[self.cp.state_new]) -oldQ,
                               5)





if __name__ == '__main__':
    unittest.main()
